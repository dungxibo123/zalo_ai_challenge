# -*- coding: utf-8 -*-
"""Prettier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i6FUk5u9HmzypnuRv7N8GzBavYpmXqOp
"""

from google.colab import drive
drive.mount('./drive')

!unzip -qq ./drive/MyDrive/ZaloAIChallenge/vocals_data.zip -d vocals_data/

!mkdir data/
!wget https://dl-challenge.zalo.ai/lyric-alignment/train.zip -P data/
!wget https://dl-challenge.zalo.ai/lyric-alignment/public_test.zip -P data/
!unzip -qq data/public_test.zip -d data/
!unzip -qq data/train.zip -d data/
#!rm -r *.zip

!unzip -qq ./drive/MyDrive/lyrics.zip -d ./data/train/

!echo '{"ẻ": 0, "6": 1, "ụ": 2, "í": 3, "3": 4, "ỹ": 5, "ý": 6, "ẩ": 7, "ở": 8, "ề": 9, "õ": 10, "7": 11, "ê": 12, "ứ": 13, "ỏ": 14, "v": 15, "ỷ": 16, "a": 17, "l": 18, "ự": 19, "q": 20, "ờ": 21, "j": 22, "ố": 23, "à": 24, "ỗ": 25, "n": 26, "é": 27, "ủ": 28, "у": 29, "ô": 30, "u": 31, "y": 32, "ằ": 33, "4": 34, "w": 35, "b": 36, "ệ": 37, "ễ": 38, "s": 39, "ì": 40, "ầ": 41, "ỵ": 42, "8": 43, "d": 44, "ể": 45, "r": 47, "ũ": 48, "c": 49, "ạ": 50, "9": 51, "ế": 52, "ù": 53, "ỡ": 54, "2": 55, "t": 56, "i": 57, "g": 58, "́": 59, "ử": 60, "̀": 61, "á": 62, "0": 63, "ậ": 64, "e": 65, "ộ": 66, "m": 67, "ẳ": 68, "ợ": 69, "ĩ": 70, "h": 71, "â": 72, "ú": 73, "ọ": 74, "ồ": 75, "ặ": 76, "f": 77, "ữ": 78, "ắ": 79, "ỳ": 80, "x": 81, "ó": 82, "ã": 83, "ổ": 84, "ị": 85, "̣": 86, "z": 87, "ả": 88, "đ": 89, "è": 90, "ừ": 91, "ò": 92, "ẵ": 93, "1": 94, "ơ": 95, "k": 96, "ẫ": 97, "p": 98, "ấ": 99, "ẽ": 100, "ỉ": 101, "ớ": 102, "ẹ": 103, "ă": 104, "o": 105, "ư": 106, "5": 107, "<unk>": 108, "<pad>": 109}' > vocab.json

!pip install huggingsound
!pip install kenlm
!pip install soundfile transformers scikit-learn click seaborn pandas pyyaml 
!pip3 install https://github.com/kpu/kenlm/archive/master.zip
!pip3 install -U datasets pyctcdecode huggingface-hub
!pip install -U transformers
!sudo apt-get install ffmpeg

#from transformers.file_utils import cached_path, hf_bucket_url
import os, zipfile
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, TrainingArguments
from datasets import load_dataset
import soundfile as sf
import torch
import kenlm
from pyctcdecode import Alphabet, BeamSearchDecoderCTC, LanguageModel
import IPython
import librosa 
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
import torch
from itertools import groupby
import glob
from huggingsound import TrainingArguments, ModelArguments, SpeechRecognitionModel, TokenSet
import json

# define function to read in sound file
def map_to_array(batch):
       
    speech, sampling_rate =  librosa.load(batch["file"], sr=16000)
    batch["speech"] = speech
    batch["sampling_rate"] = sampling_rate
    return batch

# load dummy dataset and read soundfiles
cache_dir = './cache/'

processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h', cache_dir=cache_dir)
model = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h', cache_dir=cache_dir).cuda()

sample_rate = 16000

tokens = json.loads(''.join(open('./vocab.json', 'r').readlines())).keys()
token_set = TokenSet(tokens)
print(token_set.id_by_token)
print(token_set.token_by_id)

"""
train_data = [
    {"path": "/path/to/sagan.mp3", "transcription": "extraordinary claims require extraordinary evidence"},
    {"path": "/path/to/asimov.wav", "transcription": "violence is the last refuge of the incompetent"},
]
eval_data = [
    {"path": "/path/to/sagan2.mp3", "transcription": "absence of evidence is not evidence of absence"},
    {"path": "/path/to/asimov2.wav", "transcription": "the true delight is in the finding out rather than in the knowing"},
]
"""
train_lyrics_files = glob.glob('./data/train/lyrics/*.txt')
test_lyrics_files = glob.glob('./data/public_test/lyrics/*.txt')
train_data = []
eval_data = []
for item in train_lyrics_files:
  f = open(item, 'r')
  trans = ''.join(f.readlines()).lower()
  wav_path = './data/train/songs/' + item.split('lyrics/')[-1].split('.txt')[0] + '.wav'
  train_data.append({'path': wav_path,
                     'transcription': trans
                     })
  f.close()
for item in test_lyrics_files:
  f = open(item, 'r')
  trans = ''.join(f.readlines()).lower()
  wav_path = './data/public_test/songs/' + item.split('lyrics/')[-1].split('.txt')[0] + '.wav'
  eval_data.append({'path': wav_path,
                      'transcription': trans
                      })
  f.close()

print(f"Train data: {train_data}\nEval data: {eval_data}")

for it in eval_data:
  if len(it['transcription']) == 0:
    print(it)

!mkdir model_checkpoints

tok = json.loads('{"ẻ": 0, "6": 1, "ụ": 2, "í": 3, "3": 4, "ỹ": 5, "ý": 6, "ẩ": 7, "ở": 8, "ề": 9, "õ": 10, "7": 11, "ê": 12, "ứ": 13, "ỏ": 14, "v": 15, "ỷ": 16, "a": 17, "l": 18, "ự": 19, "q": 20, "ờ": 21, "j": 22, "ố": 23, "à": 24, "ỗ": 25, "n": 26, "é": 27, "ủ": 28, "у": 29, "ô": 30, "u": 31, "y": 32, "ằ": 33, "4": 34, "w": 35, "b": 36, "ệ": 37, "ễ": 38, "s": 39, "ì": 40, "ầ": 41, "ỵ": 42, "8": 43, "d": 44, "ể": 45, "r": 47, "ũ": 48, "c": 49, "ạ": 50, "9": 51, "ế": 52, "ù": 53, "ỡ": 54, "2": 55, "t": 56, "i": 57, "g": 58, "́": 59, "ử": 60, "̀": 61, "á": 62, "0": 63, "ậ": 64, "e": 65, "ộ": 66, "m": 67, "ẳ": 68, "ợ": 69, "ĩ": 70, "h": 71, "â": 72, "ú": 73, "ọ": 74, "ồ": 75, "ặ": 76, "f": 77, "ữ": 78, "ắ": 79, "ỳ": 80, "x": 81, "ó": 82, "ã": 83, "ổ": 84, "ị": 85, "̣": 86, "z": 87, "ả": 88, "đ": 89, "è": 90, "ừ": 91, "ò": 92, "ẵ": 93, "1": 94, "ơ": 95, "k": 96, "ẫ": 97, "p": 98, "ấ": 99, "ẽ": 100, "ỉ": 101, "ớ": 102, "ẹ": 103, "ă": 104, "o": 105, "ư": 106, "5": 107, "|": 46, "<unk>": 108, "<pad>": 109}')
token_set = TokenSet(tok.keys())

output_dir = "./drive/MyDrive/ZaloAIChallenge"
model = SpeechRecognitionModel("facebook/wav2vec2-base-960h",device='cuda')
# '<unk>': 108, '<pad>': 109
'''
del model.processor.tokenizer.encoder['<s>']
del model.processor.tokenizer.encoder['</s>']
del model.processor.tokenizer.encoder['<unk>']
del model.processor.tokenizer.encoder['<pad>']
del model.processor.tokenizer.decoder[110]
del model.processor.tokenizer.decoder[111]
del model.processor.tokenizer.decoder[108]
del model.processor.tokenizer.decoder[109]

'''
print(type(model.processor.tokenizer))
print(len(model.processor.tokenizer.encoder))

#!rm -rf model_checkpoints
cache_dir = './cache'
training_args = TrainingArguments(
    overwrite_output_dir=True,
    learning_rate=1e-3,
    per_device_train_batch_size=8,
#    per_device_eval_batch_size=8,
    #load_best_model_at_end=True,
    num_train_epochs=15,
    logging_steps=2000000
)
model.finetune(
    output_dir, 
    train_data=train_data, 
    eval_data=eval_data, # the eval_data is optional
    token_set=token_set,
    data_cache_dir=cache_dir,
    training_args=training_args,

)

print(model.processor.tokenizer.vocab_size)

audio_paths = [item["path"] for item in train_data]

audio_paths

transcriptions = model.transcribe(audio_paths[:10])





pr_processor = Wav2Vec2Processor.from_pretrained('./', cache_dir=cache_dir)
pr_model = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h', cache_dir=cache_dir).cuda()